---
permalink: upgrade/upgrade_ndu_upgrade_to_entry_level_systems,html
sidebar: sidebar
keywords: ndu, nondisruptive, upgrade, AFF, FAS, systems, entry, level
summary: You must reassign the disks that belonged to node1 and node2 to node3 and node4 respectively.
---
= Nondisruptive upgrade to entry level NetApp AFF and FAS systems
:icons: font
:imagesdir: ../media/

[.lead]
You can perform a nondisruptive upgrade (NDU) to entry level NetApp AFF and FAS systems without having to purchase additional storage and cluster switches. To perform the upgrade, you first prepare the original nodes and set up the new nodes. You then reassign drives to the new nodes and migrate your data. 

In this procedure, the entry high-availability (HA) pair controllers are called node1 and node2 and the new midrange or high range HA pair controllers are called node3 and node4.

.Before you begin
Verify that you meet the following requirements:

* You have purchased a new HA pair of midrange or high-range controllers without storage.
* The new controllers have the same ONTAP release installed as the current entry system.
* You have performed a `wipeconfig` on the new controllers and they are currently waiting at the `Maintenance Mode` prompt.
* You have all the appropriate cabling on-hand.
* NVMe shelf module firmware 0121 or later is already installed.

== Migrate LIFs and data aggregates on node2 to node1
Perform the following steps to migrate the logical interfaces (LIFs) and data aggregates on node2 to node1.

.Before you begin
Verify that node2 is in advanced privilege mode.

.Steps
. Disable the cluster failover automatic giveback:
+
`storage failover modify -node _node_name_ -auto-giveback false`
. Disable auto-revert of the LIFs across both nodes of the HA pair:
+
`network interface modify -lif * -auto-revert false`  
. Display the status of all data network LIFs:
+
`network interface show -role data `
. Display the status of the cluster management LIFs:
+
`network interface show cluster_mgmt`
. Migrate all data LIFs from the storage virtual machines (SVMs) hosted on node2:
+
`network interface migrate -vserver _vserver_name_ -lif _lif_name_ -destination-node _node_name_ -destination-port _port_name_`
+
NOTE: The above command only migrates non-SAN LIFs. You cannot use this command to migrate iSCSI and FCP LIFs.
. Display the status of all data LIFs in the cluster:
+
`network interface show -role data`
. If any LIFs are down, set the administrative status of the LIFs to `up` by entering the following command, once for each LIF:
+
`network interface modify -vserver _vserver_name_ -lif _lif_name_ -home-node _node_name_ -status-admin up`
. Display the status of all data aggregates in the cluster:
+
`storage aggregate show` 
. Display failover eligibility:
+
`storage failover show`
. Migrate the data aggregates on node2 to node1:
+
`storage aggregate relocation start -aggregate _aggregate_name_ -node _node2_ -destination _node1_`   
. Display the status of all data aggregates in the cluster:
+
`storage aggregate show`
. Display the status of all data volumes in the cluster:
+
`volume show` 
. Display the ha status and ownership of epsilon:
+
`cluster show` 
. Disable cluster `ha`:
+
`cluster ha modify -configured false` 
. Display the ha status and ownership of epsilon:
+
`cluster show` 
. Halt node2:
+
`halt -node _node2_ -inhibit-takeover true`   

== Set up node4
You physically connect node2 to node4 before you reassign drives from node2 to node4 and migrate the LIFs and data aggerates on node1 to node4. 

.Before you begin
Verify that node2 is in advanced privilege mode.

.Steps
. Disconnect all the network cables from node2.
. Remove node2 from the enclosure.
. Insert the NVMe shelf module (NSM) into the bay of node2.
. Connect the NSM module to node4.
. Remove node2 and replace it with an NSM.
. Cable the node4 100GbE port (e0c or e0d) to an available port on the NSM.
. Connect the 25 GbE cabling from ports e0c & e0d on node1 to the 25 GbE onboard ports on node4.
+
NOTE: If onboard 25GbE ports are not present on node4, you must have a 25 GbE PCIe adapter to make the connection.

. For NetApp AFF A400 systems only, establish the cable connections between the system nodes:
.. If 16 Gb Fibre Channel ports are onboard, install a 25 Gb Ethernet adapter into each node for cluster connectivity during migration.
.. Connect the HA pair cables between the AFF A400 nodes using ports e0a and e0b.
.. Connect the cluster cables between the AFF A400 nodes using ports e3a and e3b.

== Reassign drives from node2 to node4
Reassign drives from node2 to node4 by performing the following steps on node4.

.Steps
. At the LOADER prompt, boot node4 into maintenance mode:
+
`boot_ontap maint` 
. Show the state of the 100 GbE interfaces:
+
`storage port show`
. Set the 100 GbE interfaces to storage ports:
+
`storage port modify -p e0c -m storage`
+
`storage port modify -p e0d -m storage`
. Verify the mode changes to the 100 GbE interfaces:
+
`storage port show`
+
Output like the following should display:
+
----
*> storage port modify -p e0c -m storage
Nov 10 16:27:23 [localhost:nvmeof.port.modify:notice]: Changing NVMe-oF port e0c to storage mode.
*>
*> storage port modify -p e0cNov 10 16:27:29 [localhost:nvmeof.subsystem.add:notice]: NVMe-oF subsystem added at address fe80::2a0:98ff:fefa:8885.

*> storage port modify -p e0d Nov 10 16:27:34 [localhost:nvmeof.port.modify:notice]: Changing NVMe-oF port e0d to storage mode.
-m storage
*>
*>
*> storage port show
Port Type Mode    Speed(Gb/s) State    Status  VLAN ID
---- ---- ------- ----------- -------- ------- -------
e0c  ENET storage 100 Gb/s    enabled  online  30
e0d  ENET storage 100 Gb/s    enabled  online  30
*> Nov 10 16:27:38 [localhost:nvmeof.subsystem.add:notice]: NVMe-oF subsystem added at address fe80::2a0:98ff:fefa:8886.
----

. Display all attached drives: 
+
`disk show -v `
. Record the local System ID.
. Reassign all drives from node2 to node4:
+
`disk reassign -s _node2_ -d _node4_ -p _node1_`
. Verify that all reassigned drives are viewable to the new System ID:
+
disk show -s _node4_System_ID_
+ 
NOTE: If drives are not viewable, *STOP* and contact technical support for assistance. 
. Exit maintenance mode: 
+
`halt`

== Migrate data aggregates, epsilon, and LIFs on node1 to node4
You prepare node4 for data migration before migrating the data aggregates, epsilon, and LIFs on node1 to node4.

.Steps
. At the LOADER prompt for the node, boot the node into boot menu:
+
`boot_ontap menu`
. Select option `6 Update flash from backup config` to restore the /var file system to node4.
+
This replaces all flash-based configuration with the last backup to disks. 
. Enter `y` to continue.
+
[NOTE]
====
As expected, the node automatically reboots to load the new copy of the /var file system. 

The node reports a System ID mismatch warning. Enter `y` to override the System ID.
====

. Migrate the cluster LIFs:
+
`set -privilege advanced`
+
`network port show`
+
[NOTE]
====
If the system cluster ports are not similar, for example, when upgrading an AFF A250 to an AFF A400, you might have to temporarily make the interfaces on node4 into cluster ports:

.. `network port modify -node _node4_ -port _port_name_ -mtu 9000 -ipspace Cluster`

.. `net int migrate -vserver Cluster -lif _cluster_LIF_  -destination-node _node4_ -destination-port _port_name_`
====   
. Migrate the data aggregates on node1 to node4:
+
`storage aggregate relocation start -aggregate-list _aggregate_list_name_ -node _node1_ -destination _node4_ -ndo-controller-upgrade true -override-destination-checks true`   
. Display the status of all data aggregates in the cluster:
+
`storage aggregate show` 
. Migrate epsilon by removing epsilon from node1 and moving to node4.
.. Remove epsilon from node1: 
+
`cluster modify -epsilon false -node _local-node_name_`
.. Move epsilon to node4: 
+
`cluster modify -epsilon true -node _partner-node_name_`

. Display the cluster status:
+
`cluster show` 
. Display all data network LIFs:
+
`network interface show -role data` 
. Migrate all data LIFs to node4:
+
`network interface migrate -vserver _vserver_name_ -lif _lif_name_ -destination-node _node_name]_` 
. Display the status of all data LIFs in the cluster:
+
`network interface show -role data `
. If any LIFs are down, set the administrative status of the LIFs to `up` by entering the following command, once for each LIF:
+
`network interface modify -vserver _vserver_name_ -lif _lif_name_ -home-node _node_name_ -status-admin up`
. Migrate the cluster management LIF:
+
`network interface migrate -vserver _vserver_name_ -lif cluster_mgmt -destination-node _node4_ -destination-port _port_name_`
+  
. Display the status of the cluster management LIF:
+
`network interface show cluster_mgmt` 
. Halt node1: 
+
`halt -node _node1_ -inhibit-takeover true -ignore-quorum-warnings true` 

== Set up node3
You physically connect node1 to node3 before you reassign drives from node1 to node3 and migrate the LIFs and data aggerates on node1 to node3. 

.Steps
. Disconnect all network cables from node1.
. Remove node1 from the enclosure.
. Insert NVMe module (NSM) into the bay of node1.
. Connect NSM module to node3.

== Reassign drives from node1 to node3
Reassign drives from node1 to node3 by performing the following steps.

.Steps
. At the LOADER prompt boot node3 into Maintenance Mode:
+
`boot_ontap maint` 
. Show the state of the 100 GbE interfaces: 
+
`storage port show`
. Set 100 GbE interfaces to storage ports:
+
`storage port modify -p e0c -m storage`
+
`storage port modify -p e0d -m storage`
. Verify the mode changes to the 100GbE interfaces:
+ 
`storage port show` 
. Display all attached drives:
+
`disk show -v` 
. Record the local System ID.
. Reassign all drives from node1 to node3:
+
`disk reassign -s _node1_ -d _node3_ -p _node2_`
. Verify that all reassigned drives are viewable to the new System ID:
+
disk show -s _node3 System ID_
+
NOTE: If drives are not viewable, *STOP* and contact technical support for assistance.
. Exit Maintenance Mode: 
+
`halt`

== Prepare node3 for data migration 
Prepare node3 for data migration and physically connect node3 to node4.

.Steps
. At the LOADER prompt of the node, boot the node into boot menu:
+
`boot_ontap menu`
. Select option `6 Update flash from backup config` to restore the /var file system to node3.
+
This replaces all flash-based configuration with the last backup to disks. 
. Enter `y` to continue.
. Allow the node to boot as normal.
+
[NOTE]
====
As expected, the node automatically reboots to load the new copy of the /var file system.

The node reports a warning that there is a System ID mismatch. Enter `y` to override the System ID.
====

. Connect node3 to node4:
.. Attach multipath high availability (MPHA) cables to the NS224 shelf to ensure redundancy.
.. Connect HA pair cables (if separate interfaces) between the nodes.
.. Connect cluster cables (if separate interfaces) between the nodes.

== Migrate LIFs and data aggregates on node4 to node3
Perform the following steps on node4 to migrate data LIFs and data aggregates on node4 to node3.

.Steps
. Migrate cluster LIFs:
+
`set -privilege advanced`
+
`network port show`
. Modify the cluster broadcast domain to include the desired cluster ports:
+
`network port broadcast-domain remove-ports -broadcast-domain _broadcast_domain_name_ -ports _port_names_`
+
`network port broadcast-domain add-ports -broadcast-domain Cluster -ports _port_names_`
+
NOTE: This step is required because ONTAP 9.8 might designate new IPspaces and one or more broadcast domains to existing physical ports that are intended for cluster connectivity.
. Modify the cluster IPspace to include the desired cluster ports and set MTU to 9000 if not already set:
+
`network port modify -node _node_name_ -port _port_name_ -mtu 9000 -ipspace Cluster`
. Display all cluster network LIFs:
+
`network interface show -role cluster` 
. Migrate all cluster network LIFs to home ports:
+
`network interface show -role cluster`
. Display all data network LIFs:
+
`network interface show -role data`
. Migrate all data LIFs to node3:
+
`network interface migrate -vserver _vserver_name_ -lif _lif_name_ -destination-node _node_name_`
. Display all data network LIFs:
+
`network interface show -role data`
+
Cluster and management LIFs cannot migrate between nodes.
.If any LIFs are down, set the administrative status of the LIFs to `up` by entering the following command, once for each LIF:
+
`network interface modify -vserver _vserver_name_ -lif _lif_name_ -home-node _node_name_ -status-admin up`
. Migrate the cluster management LIF:
+
`network interface migrate -vserver _vserver_name_ -lif cluster_mgmt -destination-node _node3_ -destination-port _port_name_`
. Display the status of the cluster management LIF:
+
`network interface show cluster_mgmt`
. Display the status of all data aggregates in the cluster:
+
`storage aggregate show`
. Enable the HA pair, storage failover, and auto-giveback: 
+
`cluster ha modify -configured true`
. Migrate data aggregates owned by node4 from to node3:
+ 
`storage aggregate relocation start -aggregate _aggregate_name_ -node _node4_ -destination _node3_`
. Display the status of all data aggregates in the cluster:
+
`storage aggregate show`
. Enable auto-revert of the network LIFs across the nodes:
+
`network interface modify -lif * -auto-revert true`
. Display cluster status:
+
`cluster show`
. Display failover eligibility: 
+
`storage failover show`
+
NOTE: You might see a Node owns aggregates belonging to another node in the cluster report in the output. If this occurs, normalize by performing a takeover and giveback from both sides of the cluster.

. Display the status of all data aggregates in the cluster:
+
`storage aggregate show`

