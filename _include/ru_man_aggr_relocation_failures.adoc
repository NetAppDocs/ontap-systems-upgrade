Aggregate relocation (ARL) might fail at different points during the upgrade.

== Check for aggregate relocation failure

During the procedure, ARL might fail in Stage 2, Stage 3, or Stage 5.

.Steps

. Enter the following command and examine the output:
+
`storage aggregate relocation show`
+
The `storage aggregate relocation show` command shows you which aggregates were successfully relocated and which ones were not, along with the causes of failure.

. Check the console for any EMS messages.
. Take one of the following actions:
+
* Take the appropriate corrective action, depending on the output of the `storage aggregate relocation show` command and the output of the EMS message.
* Force relocation of the aggregate or aggregates by using the `override-vetoes` option or the `override-destination-checks` option of the `storage aggregate relocation start` command.

+
For detailed information about the `storage aggregate relocation start`, `override-vetoes`, and `override-destination-checks` options, refer to link:other_references.html[References] to link to the _ONTAP 9 Commands: Manual Page Reference_.

== Aggregates originally on node1 are owned by node4 after completion of the upgrade

At the end of the upgrade procedure, node3 should be the new home node of aggregates that originally had node1 as the home node. You can relocate them after the upgrade.

.About this task

Aggregates might fail to relocate properly, having node1 as their home node instead of node3 under the following circumstances:

* During Stage 3, when aggregates are relocated from node2 to node3.
Some of the aggregates being relocated have node1 as their home node. For example, such an aggregate could be called aggr_node_1. If relocation of aggr_node_1 fails during Stage 3, and relocation cannot be forced, then the aggregate will be left behind on node2.
* After Stage 4, when node2 is replaced with node4.
When node2 is replaced, aggr_node_1 will come online with node4 as its home node instead of node3.

You can fix the incorrect ownership problem after Stage 6 once storage failover has been enabled by completing the following steps:

.Steps

. [[man_aggr_fail_step1]]Enter the following command to get a list of aggregates:
+
`storage aggregate show -nodes <node4> -is-home true`
+
To identify aggregates that were not correctly relocated, refer to the list of aggregates with the home owner of node1 that you obtained in the section link:prepare_nodes_for_upgrade.html[Prepare the nodes for upgrade] and compare it with output of the above command.

. [[step2]]Compare the output of <<man_aggr_fail_step1,Step 1>> with the output you captured for node1 in the section link:prepare_nodes_for_upgrade.html[Prepare the nodes for upgrade] and note any aggregates that were not correctly relocated.

. [[man_aggr_fail_Step3]]Relocate the aggregates left behind on node4:
+
`storage aggregate relocation start -node <node4> -aggr <aggr_node_1> -destination <node3>`
+
Do not use the `-ndo-controller-upgrade` parameter during this relocation.

. Enter the following command to verify that node3 is now the home owner of the aggregates:
+
`storage aggregate show -aggregate <aggr1,aggr2,aggr3...> -fields home-name`
+
`<aggr1,aggr2,aggr3...>` is the list of aggregates that had node1 as the original home owner.
+
Aggregates that do not have node3 as home owner can be relocated to node3 using the same relocation command in <<man_aggr_fail_Step3,Step 3>>.
