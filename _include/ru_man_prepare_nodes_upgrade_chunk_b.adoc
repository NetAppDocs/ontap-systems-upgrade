.. Record the model, system ID, and serial number of both nodes:
+
`system node show -node _node1,node2_ -instance`
+
NOTE: You will use the information to reassign disks and decommission the original nodes.

.. Enter the following command on both node1 and node2 and record information about the shelves, number of disks in each shelf, flash storage details, memory, NVRAM, and network cards from the output:
+
`run -node _node_name_ sysconfig`
+
NOTE: You can use the information to identify parts or accessories that you might want to transfer to node3 or node4. If you do not know if the nodes are V-Series systems or have FlexArray Virtualization software, you can learn that also from the output.

.. Enter the following command on both node1 and node2 and record the aggregates that are online on both nodes:
+
`storage aggregate show -node _node_name_ -state online`
+
NOTE: You can use this information and the information in the following substep to verify that the aggregates and volumes remain online throughout the procedure, except for the brief period when they are offline during relocation.

.. [[man_prepare_nodes_step19]]Enter the following command on both node1 and node2 and record the volumes that are offline on both nodes:
+
`volume show -node _node_name_ -state offline`
+
NOTE: After the upgrade, you will run the command again and compare the output with the output in this step to see if any other volumes have gone offline.

. Enter the following commands to see if any interface groups or VLANs are configured on node1 or node2:
+
`network port ifgrp show`
+
`network port vlan show`
+
Make note of whether interface groups or VLANs are configured on node1 or node2; you need that information in the next step and later in the procedure.

. Complete the following substeps on both node1 and node2 to confirm that physical ports can be mapped correctly later in the procedure:

.. Enter the following command to see if there are failover groups on the node other than `clusterwide`:
+
`network interface failover-groups show`
+
Failover groups are sets of network ports present on the system. Because upgrading the controller hardware can change the location of physical ports, failover groups can be inadvertently changed during the upgrade.
+
The system displays failover groups on the node, as shown in the following example:
+
....
cluster::> network interface failover-groups show

Vserver             Group             Targets
------------------- ----------------- ----------
Cluster             Cluster           node1:e0a, node1:e0b
                                      node2:e0a, node2:e0b

fg_6210_e0c         Default           node1:e0c, node1:e0d
                                      node1:e0e, node2:e0c
                                      node2:e0d, node2:e0e

2 entries were displayed.
....

.. If there are failover groups present other than `clusterwide`, record the failover group names and the ports that belong to the failover groups.

.. Enter the following command to see if there are any VLANs configured on the node:
+
`network port vlan show -node _node_name_`
+
VLANs are configured over physical ports. If the physical ports change, then the VLANs will need to be re-created later in the procedure.
+
The system displays VLANs configured on the node, as shown in the following example:
+
....
cluster::> network port vlan show

Network Network
Node    VLAN Name Port    VLAN ID MAC Address
------  --------- ------- ------- ------------------
node1   e1b-70    e1b     70      00:15:17:76:7b:69
....

.. If there are VLANs configured on the node, take note of each network port and VLAN ID pairing.

. Take one of the following actions:
+
[cols="35,65"]
|===
|If interface groups or VLANS are... |Then...

|On node1 or node2
|Complete <<man_prepare_nodes_step23,Step 23>> and <<man_prepare_nodes_step24,Step 24>>.
|Not on node1 or node2
|Go to <<man_prepare_nodes_step24,Step 24>>.
|===

. [[man_prepare_nodes_step23]] If you do not know if node1 and node2 are in a SAN or non-SAN environment, enter the following command and examine its output:
+
`network interface show -vserver _vserver_name_ -data-protocol iscsi|fcp`
+
If neither iSCSI nor FC is configured for the SVM, the command will display a message similar to the following example:
+
....
cluster::> network interface show -vserver Vserver8970 -data-protocol iscsi|fcp
There are no entries matching your query.
....
+
You can confirm that the node is in a NAS environment by using the `network interface show` command with the `-data-protocol nfs|cifs` parameters.
+
If either iSCSI or FC is configured for the SVM, the command will display a message similar to the following example:
+
....
cluster::> network interface show -vserver vs1 -data-protocol iscsi|fcp

         Logical    Status     Network            Current  Current Is
Vserver  Interface  Admin/Oper Address/Mask       Node     Port    Home
-------- ---------- ---------- ------------------ -------- ------- ----
vs1      vs1_lif1   up/down    172.17.176.20/24   node1    0d      true
....

. [[man_prepare_nodes_step24]]Verify that all the nodes in the cluster are in quorum by completing the following substeps:

.. Enter the advanced privilege level:
+
`set -privilege advanced`
+
The system displays the following message:
+
....
Warning: These advanced commands are potentially dangerous; use them only when directed to do so by NetApp personnel.
Do you wish to continue? (y or n):
....

.. Enter `y`.

.. Verify the cluster service state in the kernel, once for each node:
+
`cluster kernel-service show`
+
The system displays a message similar to the following example:
+
....
cluster::*> cluster kernel-service show

Master        Cluster       Quorum        Availability  Operational
Node          Node          Status        Status        Status
------------- ------------- ------------- ------------- -------------
node1         node1         in-quorum     true          operational
              node2         in-quorum     true          operational

2 entries were displayed.
....
+
Nodes in a cluster are in quorum when a simple majority of nodes are healthy and can communicate with each other. For more information, refer to link:other_references.html[References] to link to the _System Administration Reference_.

.. Return to the administrative privilege level:
+
`set -privilege admin`

. Take one of the following actions:
+
[cols="35,65"]
|===
|If the cluster... |Then...

|Has SAN configured
|Go to <<man_prepare_nodes_step26,Step 26>>.
|Does not have SAN configured
|Go to <<man_prepare_nodes_step29,Step 29>>.
|===

. [[man_prepare_nodes_step26]]Verify that there are SAN LIFs on node1 and node2 for each SVM that has either SAN iSCSI or FC service enabled by entering the following command and examining its output:
+
`network interface show -data-protocol iscsi|fcp -home-node _node_name_`
+
The command displays SAN LIF information for node1 and node2. The following examples show the status in the Status Admin/Oper column as up/up, indicating that SAN iSCSI and FC service are enabled:
+
....
cluster::> network interface show -data-protocol iscsi|fcp
            Logical    Status     Network                  Current   Current Is
Vserver     Interface  Admin/Oper Address/Mask             Node      Port    Home
----------- ---------- ---------- ------------------       --------- ------- ----
a_vs_iscsi  data1      up/up      10.228.32.190/21         node1     e0a     true
            data2      up/up      10.228.32.192/21         node2     e0a     true

b_vs_fcp    data1      up/up      20:09:00:a0:98:19:9f:b0  node1     0c      true
            data2      up/up      20:0a:00:a0:98:19:9f:b0  node2     0c      true

c_vs_iscsi_fcp data1   up/up      20:0d:00:a0:98:19:9f:b0  node2     0c      true
            data2      up/up      20:0e:00:a0:98:19:9f:b0  node2     0c      true
            data3      up/up      10.228.34.190/21         node2     e0b     true
            data4      up/up      10.228.34.192/21         node2     e0b     true
....
+
Alternatively, you can view more detailed LIF information by entering the following
command:
+
`network interface show -instance -data-protocol iscsi|fcp`

. Capture the default configuration of any FC ports on the original nodes by entering the following command and recording the output for your systems:
+
`ucadmin show`
+
The command displays information about all FC ports in the cluster, as shown in the following example:
+
....
cluster::> ucadmin show

                Current Current   Pending Pending   Admin
Node    Adapter Mode    Type      Mode    Type      Status
------- ------- ------- --------- ------- --------- -----------
node1   0a      fc      initiator -       -         online
node1   0b      fc      initiator -       -         online
node1   0c      fc      initiator -       -         online
node1   0d      fc      initiator -       -         online
node2   0a      fc      initiator -       -         online
node2   0b      fc      initiator -       -         online
node2   0c      fc      initiator -       -         online
node2   0d      fc      initiator -       -         online
8 entries were displayed.
....
+
You can use the information after the upgrade to set the configuration of FC ports on the new nodes.

. If you are upgrading a V-Series system or a system with FlexArray Virtualization software, capture information about the topology of the original nodes by entering the following command and recording the output:
+
`storage array config show -switch`
+
The system displays topology information, as show in the following example:
+
....
cluster::> storage array config show -switch

      LUN LUN                                  Target Side Initiator Side Initi-
Node  Grp Cnt Array Name    Array Target Port  Switch Port Switch Port    ator
----- --- --- ------------- ------------------ ----------- -------------- ------
node1 0   50  I_1818FAStT_1
                            205700a0b84772da   vgbr6510a:5  vgbr6510s164:3  0d
                            206700a0b84772da   vgbr6510a:6  vgbr6510s164:4  2b
                            207600a0b84772da   vgbr6510b:6  vgbr6510s163:1  0c
node2 0   50  I_1818FAStT_1
                            205700a0b84772da   vgbr6510a:5  vgbr6510s164:1  0d
                            206700a0b84772da   vgbr6510a:6  vgbr6510s164:2  2b
                            207600a0b84772da   vgbr6510b:6  vgbr6510s163:3  0c
                            208600a0b84772da   vgbr6510b:5  vgbr6510s163:4  2a
7 entries were displayed.
....

. [[man_prepare_nodes_step29]]Complete the following substeps:
.. Enter the following command on one of the original nodes and record the output:
+
`service-processor show -node * -instance`
+
The system displays detailed information about the SP on both nodes.
.. Confirm that the SP status is `online`.
.. Confirm that the SP network is configured.
.. Record the IP address and other information about the SP.
+
You might want to reuse the network parameters of the remote management devices, in this case the SPs, from the original system for the SPs on the new nodes.
For detailed information about the SP, refer to link:other_references.html[References] to link to the _System Administration Reference_ and the _ONTAP 9 Commands: Manual Page Reference_.

. [[man_prepare_nodes_step30]]If you want the new nodes to have the same licensed functionality as the original nodes, enter the following command to see the cluster licenses on the original system:
+
`system license show -owner *`
+
The following example shows the site licenses for cluster1:
+
....
system license show -owner *
Serial Number: 1-80-000013
Owner: cluster1

Package           Type    Description           Expiration
----------------- ------- --------------------- -----------
Base              site    Cluster Base License  -
NFS               site    NFS License           -
CIFS              site    CIFS License          -
SnapMirror        site    SnapMirror License    -
FlexClone         site    FlexClone License     -
SnapVault         site    SnapVault License     -
6 entries were displayed.
....

. Obtain new license keys for the new nodes at the  _NetApp Support Site_. Refer to link:other_references.html[References] to link to _NetApp Support Site_.
+
If the site does not have the license keys you need, contact your NetApp sales representative.

. Check whether the original system has AutoSupport enabled by entering the following command on each node and examining its output:
+
`system node autosupport show -node _node1,node2_`
+
The command output shows whether AutoSupport is enabled, as shown in the following example:
+
....
cluster::> system node autosupport show -node node1,node2

Node             State     From          To                Mail Hosts
---------------- --------- ------------- ----------------  ----------
node1            enable    Postmaster    admin@netapp.com  mailhost

node2            enable    Postmaster    -                 mailhost
2 entries were displayed.
....

. Take one of the following actions:
+
[cols="35,65"]
|===
|If the original system... |Then...

|Has AutoSupport enabled...
a|Go to <<man_prepare_nodes_step34,Step 34>>.
|Does not have AutoSupport enabled...
a|Enable AutoSupport by following the instructions in the _System Administration Reference_. (Refer to link:other_references.html[References] to link to the _System Administration Reference_.)
+
*Note:*  AutoSupport is enabled by default when you configure your storage system for the first time. Although you can disable AutoSupport at any time, you should leave it enabled. Enabling AutoSupport can significantly help identify problems and solutions should a problem occur on your storage system.

|===

. [[man_prepare_nodes_step34]]Verify that AutoSupport is configured with the correct mailhost details and recipient e-mail IDs by entering the following command on both of the original nodes and examining the output:
+
`system node autosupport show -node node_name -instance`
+
For detailed information about AutoSupport, refer to link:other_references.html[References] to link to the _System Administration Reference_ and the _ONTAP 9 Commands: Manual Page Reference_.

. [[man_prepare_nodes_step35,Step 35]] Send an AutoSupport message to NetApp for node1 by entering the following command:
+
`system node autosupport invoke -node node1 -type all -message "Upgrading node1 from platform_old to platform_new"`
+
NOTE: Do not send an AutoSupport message to NetApp for node2 at this point; you do so later in the procedure.

. [[man_prepare_nodes_step36, Step 36]] Verify that the AutoSupport message was sent by entering the following command and examining its output:
+
`system node autosupport show -node _node1_ -instance`
+
The fields `Last Subject Sent:` and `Last Time Sent:` contain the message title of the last message sent and the time the message was sent.

. If your system uses self-encrypting drives, see the Knowledge Base article https://kb.netapp.com/onprem/ontap/Hardware/How_to_tell_if_a_drive_is_FIPS_certified[How to tell if a drive is FIPS certified^] to determine the type of self-encrypting drives that are in use on the HA pair that you are upgrading. ONTAP software supports two types of self-encrypting drives:
+
--
* FIPS-certified NetApp Storage Encryption (NSE) SAS or NVMe drives
* Non-FIPS self-encrypting NVMe drives (SED)

[NOTE]
====
You cannot mix FIPS drives with other types of drives on the same node or HA pair.

You can mix SEDs with non-encrypting drives on the same node or HA pair.
====

https://docs.netapp.com/us-en/ontap/encryption-at-rest/support-storage-encryption-concept.html#supported-self-encrypting-drive-types[Learn more about supported self-encrypting drives^].
--

// 2023 APR 11, ontap-systems-upgrade-issues-64/BURT 1519747
// 2022 DEC 1, ontap-systems-upgrade-37
// 2022 MAY 13, 1476241 
// 2022 MAR 09, Clean-up 

