Before you can replace the original nodes, you must confirm that they are in an HA pair, have no missing or failed disks, can access each other's storage, and do not own data LIFs assigned to the other nodes in the cluster. You also must collect information about the original nodes and, if the cluster is in a SAN environment, confirm that all the nodes in the cluster are in quorum.

.Steps

. Confirm that each of the original nodes has enough resources to adequately support the workload of both nodes during takeover mode.
+
Refer to link:other_references.html[References] to link to _HA pair management_ and follow the _Best practices for HA pairs_ section. Neither of the original nodes should be running at more than 50 percent utilization; if a node is running at less than 50 percent utilization, it can handle the loads for both nodes during the controller upgrade.

. Complete the following substeps to create a performance baseline for the original nodes:

.. Make sure that the diagnostic user account is unlocked.
+
[IMPORTANT]
====
The diagnostic user account is intended only for low-level diagnostic purposes and should be used only with guidance from technical support.

For information about unlocking the user accounts, refer to link:other_references.html[References] to link to the _System Administration Reference_.
====

.. Refer to link:other_references.html[References] to link to the _NetApp Support Site_ and download the Performance and Statistics Collector (Perfstat Converged).
+
The Perfstat Converged tool lets you establish a performance baseline for comparison after the upgrade.

.. Create a performance baseline, following the instructions on the NetApp Support Site.

. Refer to link:other_references.html[References] to link to the _NetApp Support Site_ and open a support case on the NetApp Support Site.
+
You can use the case to report any issues that might arise during the upgrade.

. Verify that NVMEM or NVRAM batteries of node3 and node4 are charged, and charge them if they are not.
+
You must physically check node3 and node4 to see if the NVMEM or NVRAM batteries are charged. For information about the LEDs for the model of node3 and node4, refer to link:other_references.html[References] to link to the _Hardware Universe_.
+
WARNING: *Attention* Do not try to clear the NVRAM contents. If there is a need to clear the contents of NVRAM, contact NetApp technical support.

. Check the version of ONTAP on node3 and node4.
+
The new nodes must have the same version of ONTAP 9.x installed on them that is installed on the original nodes. If the new nodes have a different version of ONTAP installed, you must netboot the new controllers after you install them. For instructions on how to upgrade ONTAP, refer to link:other_references.html[References] to link to _Upgrade ONTAP_.
+
Information about the version of ONTAP on node3 and node4 should be included in the shipping boxes. The ONTAP version is displayed when the node boots up or you can boot the node to maintenance mode and run the command:
+
`version`

. Check whether you have two or four cluster LIFs on node1 and node2:
+
`network interface show -role cluster`
+
The system displays any cluster LIFs, as shown in the following example:
+
....
cluster::> network interface show -role cluster
        Logical    Status     Network          Current  Current Is
Vserver Interface  Admin/Oper Address/Mask     Node     Port    Home
------- ---------- ---------- ---------------- -------- ------- ----
node1
        clus1      up/up      172.17.177.2/24  node1    e0c     true
        clus2      up/up      172.17.177.6/24  node1    e0e     true
node2
        clus1      up/up      172.17.177.3/24  node2    e0c     true
        clus2      up/up      172.17.177.7/24  node2    e0e     true
....

. If you have two or four cluster LIFs on node1 or node2, make sure that you can ping both cluster LIFs across all the available paths by completing the following substeps:

.. Enter the advanced privilege level:
+
`set -privilege advanced`
+
The system displays the following message:
+
....
Warning: These advanced commands are potentially dangerous; use them only when directed to do so by NetApp personnel.
Do you wish to continue? (y or n):
....

.. Enter `y`.

.. Ping the nodes and test the connectivity:
+
`cluster ping-cluster -node node_name`
+
The system displays a message similar to the following example:
+
....
cluster::*> cluster ping-cluster -node node1
Host is node1
Getting addresses from network interface table...
Local = 10.254.231.102 10.254.91.42
Remote = 10.254.42.25 10.254.16.228
Ping status:
...
Basic connectivity succeeds on 4 path(s) Basic connectivity fails on 0 path(s)
................
Detected 1500 byte MTU on 4 path(s):
Local 10.254.231.102 to Remote 10.254.16.228
Local 10.254.231.102 to Remote 10.254.42.25
Local 10.254.91.42 to Remote 10.254.16.228
Local 10.254.91.42 to Remote 10.254.42.25
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
....
+
If the node uses two cluster ports, you should see that it is able to communicate on four paths, as shown in the example.

.. Return to the administrative level privilege:
+
`set -privilege admin`

. Confirm that node1 and node2 are in an HA pair and verify that the nodes are connected to each other, and that takeover is possible:
+
`storage failover show`
+
The following example shows the output when the nodes are connected to each other and
takeover is possible:
+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2
node2          node1          true     Connected to node1
....
+
Neither node should be in partial giveback. The following example shows that node1 is in partial giveback:
+
....
cluster::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- -------------------------------
node1          node2          true     Connected to node2, Partial giveback
node2          node1          true     Connected to node1
....
+
If either node is in partial giveback, use the `storage failover giveback` command to perform the giveback, and then use the `storage failover show-giveback` command to make sure that no aggregates still need to be given back. For detailed information about the commands, refer to link:other_references.html[References] to link to _HA pair management_.

. [[man_prepare_nodes_step9]]Confirm that neither node1 nor node2 owns the aggregates for which it is the current owner (but not the home owner):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`
+
If neither node1 nor node2 owns aggregates for which it is the current owner (but not the home owner), the system will return a message similar to the following example:
+
....
cluster::> storage aggregate show -node node2 -is-home false -fields owner-name,homename,state
There are no entries matching your query.
....
+
The following example shows the output of the command for a node named node2 that is the home owner, but not the current owner, of four aggregates:
+
....
cluster::> storage aggregate show -node node2 -is-home false
               -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node2        online
aggr2         node1        node2        online
aggr3         node1        node2        online
aggr4         node1        node2        online

4 entries were displayed.
....

. Take one of the following actions:
+
[cols="35,65"]
|===
|If the command in <<man_prepare_nodes_step9,Step 9>>...|Then...

|Had blank output
|Skip Step 11 and go to <<man_prepare_nodes_step12,Step 12>>.
|Had output
|Go to <<man_prepare_nodes_step11,Step 11>>.
|===

. [[man_prepare_nodes_step11]] If either node1 or node2 owns aggregates for which it is the current owner but not the home owner, complete the following substeps:

.. Return the aggregates currently owned by the partner node to the home owner node:
+
`storage failover giveback -ofnode _home_node_name_`

.. Verify that neither node1 nor node2 still owns aggregates for which it is the current owner (but not the home owner):
+
`storage aggregate show -nodes _node_name_ -is-home false -fields owner-name, home-name, state`
+
The following example shows the output of the command when a node is both the current owner and home owner of aggregates:
+
....
cluster::> storage aggregate show -nodes node1
          -is-home true -fields owner-name,home-name,state

aggregate     home-name    owner-name   state
------------- ------------ ------------ ------
aggr1         node1        node1        online
aggr2         node1        node1        online
aggr3         node1        node1        online
aggr4         node1        node1        online

4 entries were displayed.
....

. [[man_prepare_nodes_step12]] Confirm that node1 and node2 can access each other's storage and verify that no disks are missing:
+
`storage failover show -fields local-missing-disks,partner-missing-disks`
+
The following example shows the output when no disks are missing:
+
....
cluster::> storage failover show -fields local-missing-disks,partner-missing-disks

node     local-missing-disks partner-missing-disks
-------- ------------------- ---------------------
node1    None                None
node2    None                None
....
+
If any disks are missing, refer to link:other_references.html[References] to link to _Disk and aggregate management with the CLI_, _Logical storage management with the CLI_, and _HA pair management_ to configure storage for the HA pair.

. Confirm that node1 and node2 are healthy and eligible to participate in the cluster:
+
`cluster show`
+
The following example shows the output when both nodes are eligible and healthy:
+
....
cluster::> cluster show

Node                  Health  Eligibility
--------------------- ------- ------------
node1                 true    true
node2                 true    true
....

. Set the privilege level to advanced:
+
`set -privilege advanced`

. [[man_prepare_nodes_step15]] Confirm that node1 and node2 are running the same ONTAP release:
+
`system node image show -node _node1,node2_ -iscurrent true`
+
The following example shows the output of the command:
+
....
cluster::*> system node image show -node node1,node2 -iscurrent true

                 Is      Is                Install
Node     Image   Default Current Version   Date
-------- ------- ------- ------- --------- -------------------
node1
         image1  true    true    9.1         2/7/2017 20:22:06
node2
         image1  true    true    9.1         2/7/2017 20:20:48

2 entries were displayed.
....

. Verify that neither node1 nor node2 owns any data LIFs that belong to other nodes in the cluster and check the `Current Node` and `Is Home` columns in the output:
+
`network interface show -role data -is-home false -curr-node _node_name_`
+
The following example shows the output when node1 has no LIFs that are home-owned by other nodes in the cluster:
+
....
cluster::> network interface show -role data -is-home false -curr-node node1
 There are no entries matching your query.
....
+
The following example shows the output when node1 owns data LIFs home-owned by the other node:
+
....
cluster::> network interface show -role data -is-home false -curr-node node1

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
vs0
            data1      up/up      172.18.103.137/24  node1         e0d     false
            data2      up/up      172.18.103.143/24  node1         e0f     false

2 entries were displayed.
....

. If the output in <<man_prepare_nodes_step15,Step 15>> shows that either node1 or node2 owns any data LIFs home-owned by other nodes in the cluster, migrate the data LIFs away from node1 or node2:
+
`network interface revert -vserver * -lif *`
+
For detailed information about the `network interface revert` command, refer to link:other_references.html[References] to link to the _ONTAP 9 Commands: Manual Page Reference_.

. Check whether node1 or node2 owns any failed disks:
+
`storage disk show -nodelist _node1,node2_ -broken`
+
If any of the disks have failed, remove them, following instructions in the _Disk and aggregate management with the CLI_. (Refer to link:other_references.html[References] to link to _Disk and aggregate management with the CLI_.)

. Collect information about node1 and node2 by completing the following substeps and recording the output of each command:
// Clean-up, 2022-03-09
// 1476241, 2022-05-13