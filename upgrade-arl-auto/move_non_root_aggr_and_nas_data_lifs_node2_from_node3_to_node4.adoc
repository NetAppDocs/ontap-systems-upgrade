---
sidebar: sidebar
permalink: upgrade-arl-auto/move_non_root_aggr_and_nas_data_lifs_node2_from_node3_to_node4.html
keywords: move, non-root, aggregates, nas, lif, node2, node3, node4
summary: After you verify the node4 installation and before you relocate aggregates from node3 to node4, move NAS data LIFs belonging to node2.
---

= Move non-root aggregates and NAS data LIFs owned by node2 from node3 to node4
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]

// 2nd half of page 65, 66, and 67 in Pdf
After you verify the node4 installation and before you relocate aggregates from node3 to node4, you need to move the NAS data LIFs belonging to node2 that are currently on node3 from node3 to node4. You also need to verify the SAN LIFs exist on node4.

.About this task

Remote LIFs handle traffic to SAN LUNs during the upgrade procedure. Moving SAN LIFs is not necessary for cluster or service health during the upgrade. SAN LIFs are not moved unless they need to be mapped to new ports. You will verify that the LIFs are healthy and located on appropriate ports after you bring node4 online.

.Steps

. Resume the relocation operation:
+
`system controller replace resume`
+
The system performs the following tasks:

* Cluster quorum check
* System ID check
* Image version check
* Target platform check
* Network reachability check

+
The operation pauses at this stage in the network reachability check.

. Manually verify that the network and all VLANs, interface groups, and broadcast domains have been configured correctly.

. Resume the relocation operation:
+
`system controller replace resume`
+
----
To complete the "Network Reachability" phase, ONTAP network configuration must be manually adjusted to match the new physical network configuration of the hardware. This includes assigning network ports to the correct broadcast domains,creating any required ifgrps and VLANs, and modifying the home-port parameter of network interfaces to the appropriate ports.Refer to the "Using aggregate relocation to upgrade controller hardware on a pair of nodes running ONTAP 9.x" documentation, Stages 3 and 5. Have all of these steps been manually completed? [y/n]
----

. Enter `y` to continue.

. The system performs the following checks:
+
* Cluster health check
* Cluster LIF status check

+
After performing these checks, the system relocates the non-root aggregates and NAS data LIFs owned by node2 to the new controller, node4.
The system pauses once the resource relocation is complete.

. Check the status of the aggregate relocation and NAS data LIF move operations:
+
`system controller replace show-details`

. Manually verify that the non-root aggregates and NAS data LIFs have been successfully relocated to node4.
+
If any aggregates fail to relocate or are vetoed, you must take manually relocate the aggregates, or override either the vetoes or destination checks, if necessary. See the section link:relocate_failed_or_vetoed_aggr.html[Relocate failed or vetoed aggregates] for more information.

. Ensure that the SAN LIFs are on the correct ports on node4 by completing the following substeps:
+
.. Enter the following command and examine its output:
+
`network interface show -data-protocol iscsi|fcp -home-node <node4>`
+
The system returns output similar to the following example:
+
----
cluster::> net int show -data-protocol iscsi|fcp -home-node node3
        Logical    Status     Network           Current Current Is
Vserver Interface  Admin/Oper Address/Mask      Node    Port    Home
------- ---------- ---------- ----------------- ------- ------- ---
vs0
        a0a        up/down    10.63.0.53/24     node3   a0a     true
        data1      up/up      10.63.0.50/18     node3   e0c     true
        rads1      up/up      10.63.0.51/18     node3   e1a     true
        rads2      up/down    10.63.0.52/24     node3   e1b     true
vs1
        lif1       up/up      172.17.176.120/24 node3   e0c     true
        lif2       up/up      172.17.176.121/24 node3   e1a     true
----

.. If node4 has any SAN LIFs or groups of SAN LIFs that are on a port that did not exist on node2 or that need to be mapped to a different port, move them to an appropriate port on node4 by completing the following substeps:
+
... Set the LIF status to down by entering the following command:
+
`network interface modify -vserver <vserver_name> -lif <lif_name> -status-admin down`
... Remove the LIF from the port set:
+
`portset remove -vserver <vserver_name> -portset <portset_name> -port-name <port_name>`
... Enter one of the following commands:
+
* Move a single LIF by entering the following command:
+
`network interface modify -vserver <vserver_name> -lif <lif_name> -home-port <new_home_port>`
* Move all the LIFs on a single nonexistent or incorrect port to a new port by entering the following command:
+
`network interface modify {-home-port <port_on_node1> -home-node <node1> -role data} -home-port <new_home_port_on_node3>`
* Add the LIFs back to the port set:
+
`portset add -vserver <vserver_name> -portset <portset_name> -port-name <port_name>`
+
NOTE: You need to ensure that you move SAN LIFs to a port that has the same link speed as the original port.

.. Modify the status of all LIFs to `up` so the LIFs can accept and send traffic on the node by entering the following command:
+
`network interface modify -home-port <port_name> -home-node <node4> -lif data -statusadmin up`
.. Enter the following command and examine its output to verify that LIFs have been moved to the correct ports and that the LIFs have the status of `up` by entering the following command on either node and examining the output:
+
`network interface show -home-node <node4> -role data`
.. If any LIFs are down, set the administrative status of the LIFs to up by entering the following command, once for each LIF:
+
`network interface modify -vserver <vserver_name> -lif <lif_name> -status-admin up`

. Resume the operation to prompt the system to perform the required post-checks:
+
`system controller replace resume`
+
The system performs the following post-checks:
+
* Cluster quorum check
* Cluster health check
* Aggregates reconstruction check
* Aggregate status check
* Disk status check
* Cluster LIF status check
